{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets as datasets\n",
    "from torchvision import transforms as transforms\n",
    "from torchvision.models import resnet34\n",
    "from torchvision.models import ResNet34_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyLoading(Dataset):\n",
    "    def __init__(self, path, train=True, transform = None):\n",
    "        self.transform = transform\n",
    "        path = path+ (\"train/\" if train else \"test/\")\n",
    "        self.pathX = path+\"X/\"\n",
    "        self.pathY = path+\"Y/\"\n",
    "        self.data = os.listdir(self.pathX)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        f = self.data[idx]\n",
    "        img0 = cv2.imread(self.pathX+ f + \"/rgb/0.png\")\n",
    "        img1 = cv2.imread(self.pathX+ f + \"/rgb/1.png\")\n",
    "        img2 = cv2.imread(self.pathX+ f + \"/rgb/2.png\")\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        depth = np.load(self.pathX+f+\"/depth.npy\")\n",
    "        # depth=cv2.normalize(depth, None, alpha=0, beta=1,\n",
    "        #                      norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)   \n",
    "        field_id = pkl.load(open(self.pathX+f+\"/field_id.pkl\",\"rb\"))\n",
    "        Y = np.load(self.pathY+f+\".npy\")\n",
    "        return (img0, img1, img2, depth, field_id), Y*1000\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.RandomRotation(45),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                            [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LazyLoading(\"./data/\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(img0,img1,img2,depth,field_id), Y = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "res_model.eval()\n",
    "res_model.float()\n",
    "res_model.fc = nn.Linear(512,12)\n",
    "weight = res_model.conv1.weight.clone()\n",
    "res_model.conv1=  nn.Conv2d(12,64,kernel_size=7,stride=2,padding=3,bias=False)\n",
    "with torch.no_grad():\n",
    "    res_model.conv1.weight[:,:3]=weight\n",
    "    res_model.conv1.weight[:,3]=res_model.conv1.weight[:,0]\n",
    "res_model = res_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        # rgbs = torch.stack((data[0][:,:,:,0],data[0][:,:,:,1],data[0][:,:,:,2],data[1][:,:,:,0],data[1][:,:,:,1],data[1][:,:,:,2],data[2][:,:,:,0],data[2][:,:,:,1],data[2][:,:,:,2]),1)\n",
    "        data = torch.cat((data[0], data[1], data[2],data[3]),1)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        mse_loss = nn.MSELoss()\n",
    "        loss = mse_loss(output.float(), target.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 40 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/3396 (0%)]\tLoss: 4457.271973\n",
      "Train Epoch: 0 [2560/3396 (74%)]\tLoss: 1690.103760\n",
      "Train Epoch: 1 [0/3396 (0%)]\tLoss: 1175.963379\n",
      "Train Epoch: 1 [2560/3396 (74%)]\tLoss: 406.470886\n",
      "Train Epoch: 2 [0/3396 (0%)]\tLoss: 303.996521\n",
      "Train Epoch: 2 [2560/3396 (74%)]\tLoss: 153.043060\n",
      "Train Epoch: 3 [0/3396 (0%)]\tLoss: 133.720947\n",
      "Train Epoch: 3 [2560/3396 (74%)]\tLoss: 94.644348\n",
      "Train Epoch: 4 [0/3396 (0%)]\tLoss: 82.932503\n",
      "Train Epoch: 4 [2560/3396 (74%)]\tLoss: 77.679688\n",
      "Train Epoch: 5 [0/3396 (0%)]\tLoss: 83.094009\n",
      "Train Epoch: 5 [2560/3396 (74%)]\tLoss: 44.962486\n",
      "Train Epoch: 6 [0/3396 (0%)]\tLoss: 47.614555\n",
      "Train Epoch: 6 [2560/3396 (74%)]\tLoss: 29.526920\n",
      "Train Epoch: 7 [0/3396 (0%)]\tLoss: 40.342525\n",
      "Train Epoch: 7 [2560/3396 (74%)]\tLoss: 24.690664\n",
      "Train Epoch: 8 [0/3396 (0%)]\tLoss: 24.181442\n",
      "Train Epoch: 8 [2560/3396 (74%)]\tLoss: 38.084167\n",
      "Train Epoch: 9 [0/3396 (0%)]\tLoss: 30.354614\n",
      "Train Epoch: 9 [2560/3396 (74%)]\tLoss: 28.457836\n",
      "Train Epoch: 10 [0/3396 (0%)]\tLoss: 29.563353\n",
      "Train Epoch: 10 [2560/3396 (74%)]\tLoss: 19.486639\n",
      "Train Epoch: 11 [0/3396 (0%)]\tLoss: 34.050213\n",
      "Train Epoch: 11 [2560/3396 (74%)]\tLoss: 24.542816\n",
      "Train Epoch: 12 [0/3396 (0%)]\tLoss: 22.834930\n",
      "Train Epoch: 12 [2560/3396 (74%)]\tLoss: 23.273994\n",
      "Train Epoch: 13 [0/3396 (0%)]\tLoss: 19.766560\n",
      "Train Epoch: 13 [2560/3396 (74%)]\tLoss: 21.886974\n",
      "Train Epoch: 14 [0/3396 (0%)]\tLoss: 20.169195\n",
      "Train Epoch: 14 [2560/3396 (74%)]\tLoss: 13.169079\n",
      "Train Epoch: 15 [0/3396 (0%)]\tLoss: 18.737003\n",
      "Train Epoch: 15 [2560/3396 (74%)]\tLoss: 13.295845\n",
      "Train Epoch: 16 [0/3396 (0%)]\tLoss: 19.290356\n",
      "Train Epoch: 16 [2560/3396 (74%)]\tLoss: 11.019302\n",
      "Train Epoch: 17 [0/3396 (0%)]\tLoss: 14.039824\n",
      "Train Epoch: 17 [2560/3396 (74%)]\tLoss: 14.892852\n",
      "Train Epoch: 18 [0/3396 (0%)]\tLoss: 11.663394\n",
      "Train Epoch: 18 [2560/3396 (74%)]\tLoss: 8.642044\n",
      "Train Epoch: 19 [0/3396 (0%)]\tLoss: 13.601395\n",
      "Train Epoch: 19 [2560/3396 (74%)]\tLoss: 12.453227\n",
      "Train Epoch: 20 [0/3396 (0%)]\tLoss: 21.209538\n",
      "Train Epoch: 20 [2560/3396 (74%)]\tLoss: 19.831968\n",
      "Train Epoch: 21 [0/3396 (0%)]\tLoss: 19.200104\n",
      "Train Epoch: 21 [2560/3396 (74%)]\tLoss: 10.970527\n",
      "Train Epoch: 22 [0/3396 (0%)]\tLoss: 16.546684\n",
      "Train Epoch: 22 [2560/3396 (74%)]\tLoss: 14.551100\n",
      "Train Epoch: 23 [0/3396 (0%)]\tLoss: 14.748060\n",
      "Train Epoch: 23 [2560/3396 (74%)]\tLoss: 17.306234\n",
      "Train Epoch: 24 [0/3396 (0%)]\tLoss: 12.695507\n",
      "Train Epoch: 24 [2560/3396 (74%)]\tLoss: 11.392853\n",
      "Train Epoch: 25 [0/3396 (0%)]\tLoss: 11.084521\n",
      "Train Epoch: 25 [2560/3396 (74%)]\tLoss: 9.016670\n",
      "Train Epoch: 26 [0/3396 (0%)]\tLoss: 10.544730\n",
      "Train Epoch: 26 [2560/3396 (74%)]\tLoss: 8.578780\n",
      "Train Epoch: 27 [0/3396 (0%)]\tLoss: 10.390729\n",
      "Train Epoch: 27 [2560/3396 (74%)]\tLoss: 14.187713\n",
      "Train Epoch: 28 [0/3396 (0%)]\tLoss: 13.604136\n",
      "Train Epoch: 28 [2560/3396 (74%)]\tLoss: 9.072178\n",
      "Train Epoch: 29 [0/3396 (0%)]\tLoss: 5.502275\n",
      "Train Epoch: 29 [2560/3396 (74%)]\tLoss: 6.890295\n",
      "Train Epoch: 30 [0/3396 (0%)]\tLoss: 18.695633\n",
      "Train Epoch: 30 [2560/3396 (74%)]\tLoss: 19.908863\n",
      "Train Epoch: 31 [0/3396 (0%)]\tLoss: 6.667722\n",
      "Train Epoch: 31 [2560/3396 (74%)]\tLoss: 12.458057\n",
      "Train Epoch: 32 [0/3396 (0%)]\tLoss: 6.104847\n",
      "Train Epoch: 32 [2560/3396 (74%)]\tLoss: 7.664365\n",
      "Train Epoch: 33 [0/3396 (0%)]\tLoss: 5.952426\n",
      "Train Epoch: 33 [2560/3396 (74%)]\tLoss: 4.386138\n",
      "Train Epoch: 34 [0/3396 (0%)]\tLoss: 8.755476\n",
      "Train Epoch: 34 [2560/3396 (74%)]\tLoss: 9.122601\n",
      "Train Epoch: 35 [0/3396 (0%)]\tLoss: 5.129247\n",
      "Train Epoch: 35 [2560/3396 (74%)]\tLoss: 4.532745\n",
      "Train Epoch: 36 [0/3396 (0%)]\tLoss: 6.688926\n",
      "Train Epoch: 36 [2560/3396 (74%)]\tLoss: 12.659579\n",
      "Train Epoch: 37 [0/3396 (0%)]\tLoss: 8.198284\n",
      "Train Epoch: 37 [2560/3396 (74%)]\tLoss: 6.137819\n",
      "Train Epoch: 38 [0/3396 (0%)]\tLoss: 13.268049\n",
      "Train Epoch: 38 [2560/3396 (74%)]\tLoss: 5.306808\n",
      "Train Epoch: 39 [0/3396 (0%)]\tLoss: 6.715417\n",
      "Train Epoch: 39 [2560/3396 (74%)]\tLoss: 4.365595\n",
      "Train Epoch: 40 [0/3396 (0%)]\tLoss: 6.000745\n",
      "Train Epoch: 40 [2560/3396 (74%)]\tLoss: 4.343099\n",
      "Train Epoch: 41 [0/3396 (0%)]\tLoss: 6.685345\n",
      "Train Epoch: 41 [2560/3396 (74%)]\tLoss: 5.448133\n",
      "Train Epoch: 42 [0/3396 (0%)]\tLoss: 6.737923\n",
      "Train Epoch: 42 [2560/3396 (74%)]\tLoss: 6.809160\n",
      "Train Epoch: 43 [0/3396 (0%)]\tLoss: 9.385601\n",
      "Train Epoch: 43 [2560/3396 (74%)]\tLoss: 4.065317\n",
      "Train Epoch: 44 [0/3396 (0%)]\tLoss: 6.834926\n",
      "Train Epoch: 44 [2560/3396 (74%)]\tLoss: 7.114337\n",
      "Train Epoch: 45 [0/3396 (0%)]\tLoss: 4.428367\n",
      "Train Epoch: 45 [2560/3396 (74%)]\tLoss: 4.094626\n",
      "Train Epoch: 46 [0/3396 (0%)]\tLoss: 6.031551\n",
      "Train Epoch: 46 [2560/3396 (74%)]\tLoss: 4.839589\n",
      "Train Epoch: 47 [0/3396 (0%)]\tLoss: 8.035860\n",
      "Train Epoch: 47 [2560/3396 (74%)]\tLoss: 3.944284\n",
      "Train Epoch: 48 [0/3396 (0%)]\tLoss: 6.324356\n",
      "Train Epoch: 48 [2560/3396 (74%)]\tLoss: 5.674072\n",
      "Train Epoch: 49 [0/3396 (0%)]\tLoss: 6.709225\n",
      "Train Epoch: 49 [2560/3396 (74%)]\tLoss: 4.420208\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(res_model.parameters())\n",
    "for epoch in range(0, 50):\n",
    "    train(epoch, res_model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.load(\"./data/testX.pt\")\n",
    "file_ids = test_data[-1]\n",
    "depths = test_data[1]\n",
    "rgbs = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_data = torch.cat((rgbs[:,0],rgbs[:,1],rgbs[:,2],depths),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_test = torch.split(test_img_data,50,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for data in split_test:\n",
    "    output = res_model(data.to(\"cuda\"))\n",
    "    preds.append(output.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to csv file submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "outfile = 'submission.csv'\n",
    "\n",
    "output_file = open(outfile, 'w')\n",
    "\n",
    "titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n",
    "         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n",
    "# preds = []\n",
    "\n",
    "# test_data = torch.load('./test/test/testX.pt')\n",
    "# file_ids = test_data[-1]\n",
    "# rgb_data = test_data[0]\n",
    "# model.eval()\n",
    "\n",
    "# for i, data in enumerate(rgb_data):   \n",
    "#     # Please remember to modify this loop, input and output based on your model/architecture\n",
    "#     output = model(data[:1, :, :, :].to('cuda'))\n",
    "#     preds.append(output[0].cpu().detach().numpy())\n",
    "\n",
    "df = pd.concat([pd.DataFrame(file_ids), pd.DataFrame.from_records(np.concatenate(preds)/1000)], axis = 1, names = titles)\n",
    "df.columns = titles\n",
    "df.to_csv(outfile, index = False)\n",
    "print(\"Written to csv file {}\".format(outfile))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3aab7cc20887e8c1509ba5b39678cabbbfd7577f92beedb1a3654010e923107d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
